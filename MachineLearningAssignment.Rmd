---
title: "A Prediction Assignment for Machine Learning Course"
author: "September 20, 2015"
output: html_document
---

## Human Activity Recognition

Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community, especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3jfosRia6

**Data**

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. 

In this project, we use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly (marked as "A") and incorrectly in 5 different ways.

classe A - exactly according to the specification

classe B - throwing the elbows to the front

classe C - lifting the dumbbell only halfway

classe D - lowering the dumbbell only halfway

classe E - throwing the hips to the front


The goal of the project is to predict the manner in which they did the exercise. 

We can load both data files. Note that the testing data table do not have letter classification in its "classe" column and thus it is not not suitable for model computation. It was used for the assignment to check predictions of resulting models. 
We can immediately 
check "training" data dimensions. I'll start with loading packages I need.

```{r, cache=TRUE, message=FALSE}
training=read.csv("pml-training.csv", stringsAsFactors=F)
testing=read.csv("pml-testing.csv", stringsAsFactors=F)
```
Now we can look at both files closely. Note that the testing data table do not have letter classification in its "classe" column and thus it is not not suitable for model computation. It was used for the assignment to check predictions of resulting models. At this point we can ignore it and concentrate our efforts on the "training" file.  We can look at number of rows and columns, first 10 column names, and first 20 values of first column and the last "classe" column.

```{r}
dim(training)
names(training)[1:10]
training$X[1:20]
training$new_window[1:20]
training$num_window[1:20]
```
It is easy to establish that some of first columns contain names of subjects, times of recordings and other variables which do not represent physical movements but some researchers' notes. For example the very first "X" column is used to enumerate rows. Then we have users' names and so on. They should be removed for a predicting model. 

### Cleaning data

The next step is less obvious: by checking out the "training" data I've discovered a lot of empty variables which have mostly undefined values. Some of them are read as logical with nonexistent values and some as characters.
We can check the amount of defined variables in the test set and see that the number of useful predictors is fewer than 45%.
```{r}
sum(colSums(is.na(training)) !=0)

```
I will remove non numeric variables from the training set for my work together with 
the first 7 columns. In addition some of the columns with sparse data have a lot of 
undefined values (NA), and we should get rid of them as well. The new data set with 
numeric predictors will be called "trai". Note that the classifier column "classe" is 
removed after first command line as one of character columns, and I need to put it back.
```{r, cache=TRUE}
prVec=sapply(training, is.numeric)
prVec[1:7]=F
trai=training[ , prVec]
trai <- trai[, colSums(is.na(trai)) == 0] 
trai$classe=as.factor(training$classe)
```

### Setting aside a set for a cross-validation

Now let us split the data for cross-validation. I will load required packages, choose a
number for random sampling and then put 70% of the data frame into training set and the rest of it into validation set.
```{r, cache=TRUE}
library(caret); library(e1071)
seed.set = 11
inTrain = createDataPartition(y=trai$classe, p=0.7, list=F)
workTrai=trai[inTrain,]
valdTrai=trai[-inTrain, ]
```


### Choosing predictors

So we reduced the number of predictors. It is still a lot for my laptop, say "random forests" algorithm took 2 hours. Granted my laptop is not a powerful one, but what if our prediction model is supposed to work across different gadgets? It makes sense to make it less dependable on resources and to reduce the number of predictors.
Let us visualize the results and choose variables which show more difference from others.
```{r, cache=TRUE, results = 'hide'}
library(ggplot2)
library(Rmisc)
p1=qplot(workTrai[, 1], color=classe, data=workTrai, geom="density")
p2=qplot(workTrai[, 2], color=classe, data=workTrai, geom="density")
p3=qplot(workTrai[, 3], color=classe, data=workTrai, geom="density")
p4=qplot(workTrai[, 4], color=classe, data=workTrai, geom="density")
p5=qplot(workTrai[, 5], color=classe, data=workTrai, geom="density")
p6=qplot(workTrai[, 6], color=classe, data=workTrai, geom="density")
multiplot(p1,p2,p3,p4,p5,p6, cols=2)
```

We can see that data behaviors are complicated. Some of predictors are bimodal and even multimodal. It probably caused by different sizes of participants or their level of physical fit, but we do not have enough information to check it out. Let simplify the picture a little bit, loosing some of details.

```{r, cache=TRUE, results = 'hide'}
par(mfrow=c(3,4), mar=c(2,2,2,2))
sapply(workTrai[, 1:12], function(x) boxplot(x~workTrai$classe, col=2:6) )
```

We need to find ways to distinguish types of performance, labeled by letters, from each other. 

I'm going to
pick up predictors for which a median for one of types is not in middle 50% range of another. For example, we can see such condition satisfied on the picture in the lower right corner fits to this requirement and it corresponds to 12th predictor. I wrote a code choosing predictors this way.

```{r final_predictors, cache=TRUE}
numPred = names(workTrai)
numPred = numPred[-53] # Removing the "classe" variate
prefPred = rep(F, length(numPred))
classe = unique(workTrai[, "classe"])
# creating a auxilary data frame for storage of medians and quartiles
predMed = rep(0,5)
loBnd = rep(0,5)
upBnd = rep(0,5)
medns <- data.frame(classe, predMed, loBnd, upBnd)
for (i in 1:length(numPred)) {
  for (lett in classe) {
    medBounds = quantile(workTrai[workTrai[,"classe"] == lett, i], c(.25, .5, .75))
    medns[medns[,"classe"] == lett, 2] = medBounds[2]
    medns[medns[,"classe"] == lett, 3] = medBounds[1]
    medns[medns[,"classe"] == lett, 4] = medBounds[3]
  }
  # Now I need to pick up predictors for which a median for one of performance 
  # classifiers does not fall into a middle 50% range of another classifier.
  for (k in 1:length(classe)) {
    others = classe[-k]
    ind = T
    for (other in others) {
      # checking that a median is inside of other range
      ind = ind &
        ((medns[k,2] > medns[medns[,"classe"] == other, 3]) & # comparing with lower bound
           (medns[k,2] < medns[medns[,"classe"] == other, 4]))  # comparing with upper bound
    }
  } # marking choosen predictors for removal
      prefPred[i] <- prefPred[i] | (!ind)
}
```


Gathering the predictors together:
```{r, cache=TRUE, results = 'hide'}
prefVec=c(numPred[prefPred], "classe")
workdf=workTrai[ , prefVec]
```

### Random Forests Algorithm 
I will use "random forests" algorithm option from "caret" package. Since it uses random numbers I need to set up my "random number seed". Then we can look at the model and its accuracy, 
```{r random_forest, message=FALSE}
library(caret); library(e1071)
library(randomForest)
library(kernlab); library(doParallel)
seed.set=11
cl <- makeCluster(detectCores())
registerDoParallel(cl)
modrf=train(classe~., data=workdf, method="rf")
stopCluster(cl)
registerDoSEQ()
# Looking at the resulting model
modrf
```

And second, we compare predictions on the validation set:
```{r}
# Forming the testing subset of training set and computing predictions on :
valddf=valdTrai[, prefVec]
last=length(prefVec)
predictOnValdRF=predict(modrf, valddf[, -last])
confusionMatrix(valdTrai$classe, predictOnValdRF)
```
As we see on the model data and in the table the resulting accuracy is above 97%, which is quite good.

I have tried other methods as well. Naive Bayes yields not good accuracy, below 70%. Which means that the Bayes Rule formula does not work well on our data. 


### Gradient Boosting Method

```{r gradient_boosting_method, cache=TRUE, results = 'hide', message=FALSE}
 seed.set=11
library(caret); library(e1071)
library(doParallel)
cl <- makeCluster(detectCores())
registerDoParallel(cl)
modgbm=train(classe~., data=workdf, method="gbm", verbose=F)
stopCluster(cl)
registerDoSEQ()
```

We can look at the model itself and check its performance on the validation set.

```{r, message=FALSE}
modgbm
```
Here the accuracy is 90%, which is lower than for "random forests" algorithm. We compare predictions on the validation set as well:
```{r}
# Forming the testing subset of training set and computing predictions on :
valddf=valdTrai[, prefVec]
predictOnValdGBM=predict(modgbm, valddf[, -last])
confusionMatrix(valdTrai$classe, predictOnValdGBM)
```
As we see the accuracy is still around 90%.

I've tried other methods with "train" tuning, but all results were significantly lower than above.